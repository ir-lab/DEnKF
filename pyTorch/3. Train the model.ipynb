{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68deec95",
   "metadata": {},
   "source": [
    "## Training DEnKF\n",
    "DEnKF contains four sub-modules: a state transition model, an observation model, an observation noise model, and a sensor model. The entire framework is trained in an end-to-end manner via a mean squared error (MSE) loss between the ground truth state $\\hat{{\\bf x}}_{t|t}$ and the estimated state ${\\bf \\bar{x}}_{t|t}$ at every timestep. We also supervise the intermediate modules via loss gradients $\\mathcal{L}_{f_{\\pmb {\\theta}}}$ and $\\mathcal{L}_{s_{\\pmb {\\xi}}}$. Given ground truth at time $t$, we apply the MSE loss gradient calculated between $\\hat{{\\bf x}}_{t|t}$ and the output of the state transition model to $f_{\\pmb {\\theta}}$. We apply the intermediate loss gradients computed based on the ground truth observation $\\hat{{\\bf y}_t}$ and the output of the stochastic sensor model $\\tilde{{\\bf y}}_t$: \n",
    "    \\begin{align}\n",
    "    \\mathcal{L}_{f_{\\pmb {\\theta}}} =  \\| {\\bf \\bar{x}}_{t|t-1} - \\hat{{\\bf x}}_{t|t}\\|_2^2,\\ \\ \n",
    "        \\mathcal{L}_{s_{\\pmb {\\xi}}} =\\| \\tilde{{\\bf y}_t} -  \\hat{{\\bf y}_t}\\|_2^2.\n",
    "    \\end{align}\n",
    "    \n",
    "All models in the experiments were trained for 50 epochs with batch size 64, and a learning rate of $\\eta = 10^{-5}$. We chose the model with the best performance on a validation set for testing. The ensemble size of the DEnKF was set to 32 ensemble members.\n",
    "\n",
    "In this tutorial, we present and elucidate the fundamental training process of DEnkF using the `car tracking` example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3cbe9f",
   "metadata": {},
   "source": [
    "### 1. Set training parameters\n",
    "We initiate the training process by setting up the training parameters, which involve defining the dimensionality of the state and observation, determining the batch size, and selecting the model mode. This is followed by implementing the training process as a training `engine` class, as illustrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bb5f4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from dataset import CarDataset\n",
    "from model import DEnKF\n",
    "from optimizer import build_optimizer\n",
    "from optimizer import build_lr_scheduler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class Engine:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 64\n",
    "        self.dim_x = 4\n",
    "        self.dim_z = 4\n",
    "        self.num_ensemble = 32\n",
    "        self.global_step = 0\n",
    "        self.mode = 'train'\n",
    "        self.dataset = CarDataset(self.args, self.mode)\n",
    "        self.model = DEnKF(self.num_ensemble, self.dim_x, self.dim_z)\n",
    "        # Check model type\n",
    "        if not isinstance(self.model, nn.Module):\n",
    "            raise TypeError(\"model must be an instance of nn.Module\")\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if torch.cuda.is_available():\n",
    "            self.model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad285a3",
   "metadata": {},
   "source": [
    "Then, we proceed to define the actual training script, which includes declaring the optimizer and learning scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4ec8fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function, dataloader, optimizer for training the model\n",
    "def train(self):\n",
    "    mse_criterion = nn.MSELoss()\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        self.dataset, batch_size=self.batch_size, shuffle=True, num_workers=1\n",
    "    )\n",
    "    pytorch_total_params = sum(\n",
    "        p.numel() for p in self.model.parameters() if p.requires_grad\n",
    "    )\n",
    "    print(\"Total number of parameters: \", pytorch_total_params)\n",
    "\n",
    "    # Create optimizer\n",
    "    optimizer_ = build_optimizer(\n",
    "        [self.model],\n",
    "        self.args.network.name,\n",
    "        self.args.optim.optim,\n",
    "        self.args.train.learning_rate,\n",
    "        self.args.train.weight_decay,\n",
    "        self.args.train.adam_eps,\n",
    "    )\n",
    "\n",
    "    # Create LR scheduler\n",
    "    if self.args.mode.mode == \"train\":\n",
    "        num_total_steps = self.args.train.num_epochs * len(dataloader)\n",
    "        scheduler = build_lr_scheduler(\n",
    "            optimizer_,\n",
    "            self.args.optim.lr_scheduler,\n",
    "            self.args.train.learning_rate,\n",
    "            num_total_steps,\n",
    "            self.args.train.end_learning_rate,\n",
    "        )\n",
    "    # Epoch calculations\n",
    "    steps_per_epoch = len(dataloader)\n",
    "    num_total_steps = self.args.train.num_epochs * steps_per_epoch\n",
    "    epoch = self.global_step // steps_per_epoch\n",
    "    duration = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd00af0",
   "metadata": {},
   "source": [
    "### 2. Training with curricula\n",
    "Within the main training loop, it is useful to define different training curricula. Our proposed framework is designed to be modular, providing the flexibility to use individual components independently. However, it is important to acknowledge that various learning tasks may require specific curricula. For instance, complex visual tasks might demand a longer training period for the sensor model before it can be seamlessly integrated into end-to-end learning. Consequently, there is currently no universal curriculum that ensures optimal performance of all sub-modules in any given scenario. In the `car tracking` example, we pretrining the sensor model for `10 epoch` first then conduct end-to-end training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e337c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self):    \n",
    "    ####################################################################################################\n",
    "    # MAIN TRAINING LOOP\n",
    "    ####################################################################################################\n",
    "    while epoch < self.args.train.num_epochs:\n",
    "        step = 0\n",
    "        for data in dataloader:\n",
    "            # collect data from data loader\n",
    "            data = [item.to(self.device) for item in data]\n",
    "            state_ensemble = data[1]\n",
    "            state_pre = data[0]\n",
    "            obs = data[3]\n",
    "            state_gt = data[2]\n",
    "\n",
    "            # init optimizer\n",
    "            optimizer_.zero_grad()\n",
    "\n",
    "            # forward pass\n",
    "            input_state = (state_ensemble, state_pre)\n",
    "            obs_action = obs\n",
    "            output = self.model(obs_action, input_state)\n",
    "\n",
    "            final_est = output[1]  # -> final estimation\n",
    "            inter_est = output[2]  # -> state transition output\n",
    "            obs_est = output[3]  # -> learned observation\n",
    "            hx = output[5]  # -> observation model output\n",
    "\n",
    "            # calculate loss\n",
    "            loss_1 = mse_criterion(final_est, state_gt)\n",
    "            loss_2 = mse_criterion(inter_est, state_gt)\n",
    "            loss_3 = mse_criterion(obs_est, state_gt)\n",
    "            loss_4 = mse_criterion(hx, state_gt)\n",
    "            \n",
    "            # define training curricula\n",
    "            if epoch <= 10:\n",
    "                final_loss = loss_3\n",
    "            else:\n",
    "                final_loss = loss_1 + loss_2 + loss_3 + loss_4\n",
    "\n",
    "\n",
    "            # back prop\n",
    "            final_loss.backward()\n",
    "            optimizer_.step()\n",
    "            current_lr = optimizer_.param_groups[0][\"lr\"]\n",
    "\n",
    "            # verbose\n",
    "            if self.global_step % self.args.train.log_freq == 0:\n",
    "                string = \"[epoch][s/s_per_e/gs]: [{}][{}/{}/{}], lr: {:.12f}, loss: {:.12f}\"\n",
    "                self.logger.info(\n",
    "                    string.format(\n",
    "                        epoch,\n",
    "                        step,\n",
    "                        steps_per_epoch,\n",
    "                        self.global_step,\n",
    "                        current_lr,\n",
    "                        final_loss,\n",
    "                    )\n",
    "                )\n",
    "                if np.isnan(final_loss.cpu().item()):\n",
    "                    self.logger.warning(\"NaN in loss occurred. Aborting training.\")\n",
    "                    return -1\n",
    "\n",
    "            step += 1\n",
    "            self.global_step += 1\n",
    "            if scheduler is not None:\n",
    "                scheduler.step(self.global_step)\n",
    "\n",
    "        # Save a model based of a chosen save frequency\n",
    "        if self.global_step != 0 and (epoch + 1) % self.args.train.save_freq == 0:\n",
    "            checkpoint = {\n",
    "                \"global_step\": self.global_step,\n",
    "                \"model\": self.model.state_dict(),\n",
    "                \"optimizer\": optimizer_.state_dict(),\n",
    "            }\n",
    "            torch.save(\n",
    "                checkpoint,\n",
    "                os.path.join(\n",
    "                    self.args.train.log_directory,\n",
    "                    self.args.train.model_name,\n",
    "                    \"model-{}\".format(self.global_step),\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        # online evaluation\n",
    "        if (\n",
    "            self.args.mode.do_online_eval\n",
    "            and self.global_step != 0\n",
    "            and epoch + 1 >= 10\n",
    "            and (epoch + 1) % self.args.train.eval_freq == 0\n",
    "        ):\n",
    "            time.sleep(0.1)\n",
    "            self.model.eval()\n",
    "            self.test()\n",
    "            self.model.train()\n",
    "        # Update epoch\n",
    "        epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb673ec0",
   "metadata": {},
   "source": [
    "### 3. Test\n",
    "Similar to the training scripts, the `test()` function utilizes the testing dataloader to sequentially feed observation data into the trained filter. The model begins with an initial state and continuously tracks the state recursively while considering only the observations provided. The test output is saved as a pickle file, utilizing a dictionary format for storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cac762a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(self):\n",
    "    test_dataset = CarDataset(self.args, \"test\")\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=1, shuffle=False, num_workers=1\n",
    "    )\n",
    "    step = 0\n",
    "    data_out = {}\n",
    "    data_save = []\n",
    "    ensemble_save = []\n",
    "    gt_save = []\n",
    "    obs_save = []\n",
    "    for data in test_dataloader:\n",
    "        data = [item.to(self.device) for item in data]\n",
    "        state_ensemble = data[1]\n",
    "        state_pre = data[0]\n",
    "        obs = data[3]\n",
    "        state_gt = data[2]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if step == 0:\n",
    "                ensemble = state_ensemble\n",
    "                state = state_pre\n",
    "            else:\n",
    "                ensemble = ensemble\n",
    "                state = state\n",
    "            input_state = (ensemble, state)\n",
    "            obs_action = obs\n",
    "            output = self.model(obs_action, input_state)\n",
    "\n",
    "            ensemble = output[0]  # -> ensemble estimation\n",
    "            state = output[1]  # -> final estimation\n",
    "            obs_p = output[3]  # -> learned observation\n",
    "\n",
    "            final_ensemble = ensemble  # -> make sure these variables are tensor\n",
    "            final_est = state\n",
    "            obs_est = obs_p\n",
    "\n",
    "            final_ensemble = final_ensemble.cpu().detach().numpy()\n",
    "            final_est = final_est.cpu().detach().numpy()\n",
    "            obs_est = obs_est.cpu().detach().numpy()\n",
    "            state_gt = state_gt.cpu().detach().numpy()\n",
    "\n",
    "            data_save.append(final_est)\n",
    "            ensemble_save.append(final_ensemble)\n",
    "            gt_save.append(state_gt)\n",
    "            obs_save.append(obs_est)\n",
    "            step = step + 1\n",
    "\n",
    "    data_out[\"state\"] = data_save\n",
    "    data_out[\"ensemble\"] = ensemble_save\n",
    "    data_out[\"gt\"] = gt_save\n",
    "    data_out[\"observation\"] = obs_save\n",
    "\n",
    "    save_path = os.path.join(\n",
    "        self.args.train.eval_summary_directory,\n",
    "        self.args.train.model_name,\n",
    "        \"eval-result-{}.pkl\".format(self.global_step),\n",
    "    )\n",
    "\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        pickle.dump(data_out, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d010bf19",
   "metadata": {},
   "source": [
    "### 4. Putting everything together\n",
    "Within our repository, we employ a `.yaml` file to initialize the parameters and manage all relevant training or testing setups. The implementation of the engine class is located in `/pyTorch/engine.py`, while the `.yaml` file can be found in `/pyTorch/config/car_tracking.yaml`. The following demonstrates how to execute the training script via the command line along with the corresponding logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcac28d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-10 14:48:02,786 MainThread INFO DEnKF - mode:\n",
      "  dist_backend: nccl\n",
      "  dist_url: tcp://127.0.0.1:2345\n",
      "  do_online_eval: True\n",
      "  gpu: None\n",
      "  mode: train\n",
      "  multiprocessing_distributed: False\n",
      "  num_threads: 1\n",
      "  parameter_path: \n",
      "  rank: 0\n",
      "  world_size: 1\n",
      "network:\n",
      "  activation_function: ELU\n",
      "  encoder: resnet50\n",
      "  name: DEnKF\n",
      "optim:\n",
      "  lr_scheduler: polynomial_decay\n",
      "  optim: adamw\n",
      "test:\n",
      "  checkpoint_path: \n",
      "  data_path: ./dataset/car_dataset_test.pkl\n",
      "  dataset: car_dataset\n",
      "  dim_a: \n",
      "  dim_x: 4\n",
      "  dim_z: 4\n",
      "  eigen_crop: False\n",
      "  garg_crop: False\n",
      "  input_height: None\n",
      "  input_size: \n",
      "  input_width: None\n",
      "  model_name: DEnKF\n",
      "  num_ensemble: 32\n",
      "train:\n",
      "  adam_eps: 0.001\n",
      "  batch_size: 128\n",
      "  checkpoint_path: \n",
      "  data_path: ./dataset/car_dataset_train.pkl\n",
      "  dataset: car_dataset\n",
      "  dim_a: \n",
      "  dim_x: 4\n",
      "  dim_z: 4\n",
      "  end_learning_rate: -1.0\n",
      "  eval_freq: 5\n",
      "  eval_summary_directory: ./experiments/\n",
      "  input_size: \n",
      "  learning_rate: 0.0001\n",
      "  log_directory: ./experiments\n",
      "  log_freq: 100\n",
      "  loss: mse\n",
      "  loss_weights: [0.5, 0.5]\n",
      "  model_name: DEnKF\n",
      "  multitask: False\n",
      "  num_ensemble: 32\n",
      "  num_epochs: 30\n",
      "  random_rotate: False\n",
      "  retrain: False\n",
      "  save_freq: 10000\n",
      "  seed: 0\n",
      "  segment_classes: 55\n",
      "  steps_per_alpha_update: 100\n",
      "  task_balance: None\n",
      "  use_right: False\n",
      "  variance_focus: 0.85\n",
      "  weight_decay: 0.01\n",
      "2023-07-10 14:48:02,786 MainThread INFO DEnKF - check mode - train\n",
      "2023-07-10 14:48:02,786 MainThread WARNING DEnKF - This logging directory already exists: ./experiments/DEnKF. Over-writing current files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters:  2072664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-10 14:48:06,324 MainThread INFO DEnKF - [epoch][s/s_per_e/gs]: [0][0/464/0], lr: 0.000100000000, loss: 0.486620664597\n",
      "2023-07-10 14:49:20,899 MainThread INFO DEnKF - [epoch][s/s_per_e/gs]: [0][100/464/100], lr: 0.000099417894, loss: 0.292591094971\n"
     ]
    }
   ],
   "source": [
    "os.system('python train.py --config ./config/car_tracking.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52de722",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
